<!DOCTYPE HTML>
<html lang="en">
  <head>

    <!-- Google tag (gtag.js) -->
    <script async
      src="https://www.googletagmanager.com/gtag/js?id=G-Z80P1GGMNH"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-Z80P1GGMNH');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Seongwoong Cho</title>

    <meta name="author" content="Seongwoong Cho">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico"
      type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table
      style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Seongwoong Cho 
                    </p>
                    <p style="text-align:center">
                      seongwoongjo [at] kaist.ac.kr
                    </p>
                    <p>
                      I am a M.S. student at <a
                        href="https://www.kaist.ac.kr/en/">KAIST</a> School of
                      Computing, advised by <a
                        href="https://maga33.github.io/">Seunghoon Hong</a>.
                    </p>
                    <p>
                      I am interested in data-efficient generalization to 
                      out-of-distribution (OOD) tasks. Specifically, I have been focusing on 
                      building few-shot generalists that can generalize to both unseen tasks 
                      and unseen domains. I am excited to explore technologies across various fields 
                      in Machine Learning, including Computer Vision, Natural Language Processing, and Reinforcement Learning.
                    </p>
                    <p style="text-align:center">
                      <a href="data/seongwoongcho_cv_20240930.pdf">CV</a>
                      &nbsp;/&nbsp;
                      <a
                        href="https://scholar.google.co.kr/citations?user=wl5DfXoAAAAJ&hl=en">Google
                        Scholar</a>
                      &nbsp;/&nbsp;
                      <a href="https://github.com/seongwoongcho">GitHub</a>
                      &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/seongwoongcho">LinkedIn</a>
                    </p>
                  </td>
                  <td
                    style="padding:2.5%;width:30%;max-width:40%;vertical-align:middle">
                    <a href="images/seongwoongcho-profile.jpeg"><img
                        style="width:180px; height:180px; object-fit: cover"
                        alt="profile photo"
                        src="images/seongwoongcho-profile.jpeg"
                        class="hoverZoomLink"></a>
                  </td>
                </tr>
              </tbody></table>

            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>News</h2>
                    <p>
                      <strong>Sep 2024:</strong> Meta-Controller was accepted to NeurIPS 2024. <br>
                    </p>
                    </td>
                </tr>
              </tbody>
            </table>
            
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:0px;">
                        <h2>Research</h2>
                        <p>
                          * denotes equal contribution.
                        </p>
                      </td>
                    </tr>
                  </tbody></table>
                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <span class="papertitle">Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in Continuous Control</span>
                        <br>
                        <strong>Seongwoong Cho*</strong>, Donggyun Kim*,
                        Jinwoo Lee, Seunghoon Hong
                        <br>
                        <em>Accepted to NeurIPS</em>, 2024
                        <br>
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2404.18459" style="color: inherit; text-decoration: none;" id="VTM">
                          <span class="papertitle">Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild</span>
                        </a>
                        <br>
                        Donggyun Kim, <strong>Seongwoong Cho</strong>, Semin Kim, Chong Luo, Seunghoon Hong
                        <br>
                        <em>ECCV</em>, 2024 &nbsp<font
                          color="red"><strong>(ORAL Presentation)</strong></font>
                        <br>
                        <a href="https://arxiv.org/abs/2404.18459">paper</a> /
                        <!-- <a -->
                        <!--   href="https://github.com/GitGyun/chameleon">code</a> -->
                        <!-- <br> -->
                      </td>
                    </tr>

                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2303.14969" style="color: inherit; text-decoration: none;" id="VTM">
                          <span class="papertitle">Universal Few-shot Learning
                            of
                            Dense Prediction Tasks with Visual Token
                            Matching</span>
                        </a>
                        <br>
                        Donggyun Kim, Jinwoo Kim, <strong>Seongwoong Cho</strong>,
                        Chong Luo, Seunghoon Hong
                        <br>
                        <em>ICLR</em>, 2023 &nbsp<font
                          color="red"><strong>(Outstanding Paper
                            Award)</strong></font>
                        <br>
                        <a href="https://arxiv.org/abs/2303.14969">paper</a> /
                        <a
                          href="https://github.com/GitGyun/visual_token_matching">code</a>
                        <br>
                      </td>
                    </tr>
                    
                    <tr>
                      <td
                        style="padding:20px;width:100%;vertical-align:middle;padding-top:10px;padding-bottom:10px;">
                        <a href="https://arxiv.org/abs/2110.14953" style="color: inherit; text-decoration: none;" id="VTM">
                          <span class="papertitle">Multi-task Neural processes</span>
                        </a>
                        <br>
                        Donggyun Kim, <strong>Seongwoong Cho</strong>,
                        Wonkwang Lee, Seunghoon Hong
                        <br>
                        <em>ICLR</em>, 2022</font>
                        <br>
                        <a href="https://arxiv.org/abs/2110.14953">paper</a> /
                        <a
                          href="https://github.com/GitGyun/multi_task_neural_processesg">code</a>
                        <br>
                      </td>
                    </tr>
                    
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Experience</h2>
                        <p>
                          <strong>Waddle Inc.</strong> <br>
                          AI developer, 2020.07-2020.12
                        </p>
                        <p>
                          <strong>Pavilion Inc.</strong> <br>
                          Co-founder and AI developer, 2019.04-2020.04
                        </p>
                        <p>
                          <strong>NCSOFT ASR Group</strong> <br>
                            Intern, 2018.12-2019.02
                        </p>


                      </td>
                    </tr>
                  </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0"
                  cellpadding="20"><tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Honors</h2>
                        <p>
                          <strong>Outstanding Paper Award,</strong> ICLR 2023
                          (as
                          a coauthor) <br>
                          
                          <strong>Silver Prize,</strong> Samsung Humantech Paper
                          Award, 2023 (as a coauthor) <br>

                          <strong>1st Place,</strong> KAIST-Qualcomm Innovation Awards' Multimodal Emotional Recognition Competition, 2020<br>
                          <strong> 17th Place (over 400 teams), </strong> NIPA AI Online Competition, 2019<br>
                          <strong> 16th Place (over 200 teams), </strong> NIPA AIStarthon Competition, 2019<br>
                          <strong>1st Place,</strong> KAIST-Qualcomm Innovation Awards' Speech emotional recognition competition, 2019<br>
                          <strong>Development Award (4th Place) ,</strong> E*5 KAIST, 2019 <br>
                          <strong>2nd Place,</strong> SNU Startup Camp, 2017 <br>
                          <strong>Recipient,</strong> KAIST Dean's List, Spring 2017 / Fall 2020
 
                        </p>
                      </td>
                    </tr>
                  </tbody></table>
                
                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td style="padding:0px">
                        <br>
                        <p style="text-align:left;font-size:small;">
                          Last updated: Sep 2024
                        </p>
                      </td>
                      <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                          Built from <a href="https://jonbarron.info/"
                            style="font-size:small;">Jon Barron</a>'s academic
                          website
                        </p>
                      </td>
                    </tr>
                  </tbody></table>
              </td>
            </tr>
          </table>
        </body>
      </html>
